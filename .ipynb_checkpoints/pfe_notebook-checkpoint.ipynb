{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8094fb23-9b9a-4cd7-bd08-f4fef0526a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/oury/.local/lib/python3.10/site-packages (4.20.0)\n",
      "Requirement already satisfied: filelock in /home/oury/.local/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/oury/.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/oury/.local/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /home/oury/.local/lib/python3.10/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/oury/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oury/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oury/.local/lib/python3.10/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oury/.local/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oury/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oury/.local/lib/python3.10/site-packages (from requests->transformers) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/oury/.local/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: responses<0.19 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (0.7.0)\n",
      "Requirement already satisfied: aiohttp in /home/oury/.local/lib/python3.10/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in /home/oury/.local/lib/python3.10/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/oury/.local/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in /home/oury/.local/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /home/oury/.local/lib/python3.10/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/oury/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/oury/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oury/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oury/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oury/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oury/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/oury/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/oury/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/oury/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m561.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /home/oury/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/oury/.local/lib/python3.10/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/lib/python3/dist-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.0\n",
      "    Uninstalling scikit-learn-1.1.0:\n",
      "      Successfully uninstalled scikit-learn-1.1.0\n",
      "Successfully installed scikit-learn-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/oury/.local/lib/python3.10/site-packages (0.1.97)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install -U scikit-learn\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9cf55d-b306-4a3b-94eb-eec6840dcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/makcedward/nlpaug.git\n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c09ec7-ba9f-4a87-81e0-c09363bfe3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nlpaug.util.file.download import DownloadUtil\n",
    "# DownloadUtil.download_word2vec(dest_dir='.') # Download word2vec model\n",
    "# DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # Download GloVe model\n",
    "# DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # Download fasttext model\n",
    "\n",
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d9899b-e0f8-4c08-a442-4d11f7121389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import sys\n",
    "import getopt\n",
    "import bleach\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "# import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def absolute_file_paths(directory):\n",
    "    all_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            one_path = os.path.abspath(os.path.join(root, f))\n",
    "            all_paths.append(one_path)\n",
    "    return all_paths\n",
    "\n",
    "def dirExists(directory):\n",
    "    if os.path.exists(directory):\n",
    "        return True\n",
    "    elif os.access(os.path.dirname(directory), os.W_OK):\n",
    "        print(\"Cannot access the directory\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Directory does not exist.\")\n",
    "        return False\n",
    "\n",
    "def get_all_filenames_with_abspath(directory):\n",
    "    if dirExists(directory):\n",
    "        all_paths = absolute_file_paths(directory)\n",
    "        return all_paths\n",
    "    else:\n",
    "        sys.exit()\n",
    "\n",
    "def is_truth_text_file(f):\n",
    "    return 'truth.txt' in f \n",
    "\n",
    "def get_truth_text_files(all_paths):\n",
    "    return [f for f in all_paths if is_truth_text_file(f)]\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    # text = re.sub('<.*?>+', ' ', text)\n",
    "    # text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub('\\t', ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub(' +', ' ', text) # remove extra spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def summary_to_df(output_filename, f, all_paths, col):\n",
    "    path = output_filename.strip().split(\"/\")\n",
    "    path = '/'.join(path[0:-1])\n",
    "    \n",
    "    gender = {'M': 0, 'F': 1}\n",
    "    age_group = {'18-24': 0, '25-34': 1, '35-49': 2, '50-XX': 3, '50-64': 3, 'XX-XX': None}\n",
    "    \n",
    "    file = open(f, 'r')\n",
    "    all_data = []\n",
    "\n",
    "    for line in file:\n",
    "        a = line.strip().split(\":::\")\n",
    "        filename = path + \"/\" + a[0] + \".xml\"\n",
    "        this_gender = gender[a[1]]\n",
    "        this_age_group = age_group[a[2]]\n",
    "        this_extroverted = (a[3])\n",
    "        this_stable = float(a[4])\n",
    "        this_agreeable = float(a[5])\n",
    "        this_open = float(a[6])\n",
    "        this_conscientious = float(a[7])\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(filename)\n",
    "            print(\"Parsing file: \", filename)\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: \", e)\n",
    "        else:\n",
    "            all_docs = tree.getroot().findall(\"document\")\n",
    "            all_text = []\n",
    "\n",
    "            for doc in all_docs:               \n",
    "                clean = bleach.clean(doc.text, tags=[], strip=True)\n",
    "                clean = clean_text(clean)\n",
    "\n",
    "                all_text.append(clean)\n",
    "\n",
    "            # all_text = all_text.encode('utf-8')\n",
    "\n",
    "            data = [this_gender, this_age_group, this_extroverted, \\\n",
    "                    this_stable, this_agreeable, this_open, this_conscientious,\n",
    "                    all_text]\n",
    "\n",
    "            all_data.append(data)\n",
    "\n",
    "    return pd.DataFrame(all_data, columns=col)\n",
    "\n",
    "\n",
    "input_dir = '.'\n",
    "print(input_dir)\n",
    "all_paths = get_all_filenames_with_abspath(input_dir)\n",
    "col = [\"gender\", \"age\", \"extroverted\", \"stable\", \"agreeable\", \\\n",
    "            \"open\", \"conscientious\", \"text\"]\n",
    "    \n",
    "# langs = [\"english\", \"french\", \"spanish\", \"italian\"]\n",
    "all_truth_text = get_truth_text_files(all_paths)\n",
    "print(all_truth_text)\n",
    "\n",
    "\n",
    "for i, f in enumerate(all_truth_text):\n",
    "    path = f.strip().split(\"/\")\n",
    "    print(\"path: \", path)\n",
    "\n",
    "    output_filename = '/'.join(path[0:-1]) + '/summary-' + path[-1]\n",
    "    print(\"Summary\")\n",
    "    print(output_filename)\n",
    "    dtf = summary_to_df(output_filename, f, all_paths, col)\n",
    "    print(type(dtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01d5629-6a36-4def-bbda-a228eda11cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24186/1784377167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtf' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, TrainingArguments, Trainer)\n",
    "\n",
    "dtf.drop('age', axis=1, inplace=True)\n",
    "dtf.drop('gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1554c-76bc-47db-9b65-c3077ee21c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can't wait for my paycheck!!! I'm jonsing for some pizza hard core!! #stonergoals \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf['text'][10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0700f-1875-4251-9ea1-32814cb18cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "# DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='./model_dir') # Download fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68cd1b1-2b40-4740-a06b-126f79ed05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38868cac-4e2e-4844-b771-ccb4d76053a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug.augmenter.char as nac\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# import nlpaug.augmenter.sentence as nas\n",
    "# import nlpaug.flow as nafc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ef142-30de-4a81-b121-b98aed09a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in dtf['text']:\n",
    "#     aug = naw.WordEmbsAug(\n",
    "#         model_type='fasttext', model_path='./model_dir/wiki-news-300d-1M.vec',\n",
    "#         action=\"substitute\")\n",
    "#     augmented_text = aug.augment(text)\n",
    "#     print(\"Original:\")\n",
    "#     print(text)\n",
    "#     print(\"Augmented Text:\")\n",
    "#     print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe7903-8ec3-4d85-862f-3eb3ba940869",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf['extroverted'] = pd.to_numeric(dtf['extroverted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c08a93-d92d-458b-a17e-a2e3f3ea337f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extroverted</th>\n",
       "      <th>stable</th>\n",
       "      <th>agreeable</th>\n",
       "      <th>open</th>\n",
       "      <th>conscientious</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>How to Test Your Startup Idea for \\t\\t usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Everyday I come up with a new optimum solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>username username username username IfMyNameW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I wish we could talk forever \\t\\t things I wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>icarly \\t\\tthuis lt \\t\\tHomework finished \\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>If you fight for a cause then fight for a cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>We ain t picture perfect but we worth the pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Nasa to turn ISS into perfect Earth observing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>lisent username Nadim talks to username about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT username Get students to engage with writte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     extroverted  stable  agreeable  open  conscientious  \\\n",
       "0              1       1          1     1              1   \n",
       "1              1       1          1     0              1   \n",
       "2              1       1          1     1              1   \n",
       "3              1       1          0     1              1   \n",
       "4              0       0          1     1              1   \n",
       "..           ...     ...        ...   ...            ...   \n",
       "147            1       1          1     1              1   \n",
       "148            1       1          0     1              1   \n",
       "149            1       0          0     1              1   \n",
       "150            1       1          1     1              1   \n",
       "151            1       1          1     1              1   \n",
       "\n",
       "                                                  text  \n",
       "0    How to Test Your Startup Idea for \\t\\t usernam...  \n",
       "1    Everyday I come up with a new optimum solution...  \n",
       "2     username username username username IfMyNameW...  \n",
       "3    I wish we could talk forever \\t\\t things I wan...  \n",
       "4    icarly \\t\\tthuis lt \\t\\tHomework finished \\t\\t...  \n",
       "..                                                 ...  \n",
       "147   If you fight for a cause then fight for a cau...  \n",
       "148  We ain t picture perfect but we worth the pict...  \n",
       "149  Nasa to turn ISS into perfect Earth observing ...  \n",
       "150  lisent username Nadim talks to username about ...  \n",
       "151  RT username Get students to engage with writte...  \n",
       "\n",
       "[152 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['extroverted', 'stable', 'agreeable', 'open', 'conscientious']\n",
    "dtf[cols] = dtf[cols].applymap(lambda x: int(x > 0))\n",
    "dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bc687-6500-4bde-a2e9-c8058d04ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(dtf)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6323b8-c794-498b-bb16-667f4130aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['extroverted', 'stable', 'agreeable', 'open', 'conscientious', 'text'],\n",
       "        num_rows: 121\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['extroverted', 'stable', 'agreeable', 'open', 'conscientious', 'text'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40dcf6f-4cbb-4772-a5ed-ad229dd72bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['extroverted', 'stable', 'agreeable', 'open', 'conscientious']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3238e-088e-4d62-9cbe-6fc942c01a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'extroverted', 1: 'stable', 2: 'agreeable', 3: 'open', 4: 'conscientious'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028dcd1-2a62-4cb9-8995-55b92bd7dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "# import torch\n",
    "# from transformers import PegasusTokenizer, BigBirdPegasusForSequenceClassification\n",
    "\n",
    "# num_labels = 5\n",
    "# model = BigBirdPegasusForSequenceClassification.from_pretrained(\n",
    "#     \"hf-internal-testing/tiny-random-bigbird_pegasus\", num_labels=num_labels, \n",
    "#     problem_type=\"multi_label_classification\",\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True)\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bigbird_pegasus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bc274-3d99-47a1-87ef-1acebfaf4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "\n",
    "num_labels=5\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\",\n",
    "                                                            num_labels=num_labels, problem_type=\"multi_label_classification\", \n",
    "                                                            id2label=id2label,\n",
    "                                                            label2id=label2id,\n",
    "                                                            ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84449ea-aad1-4ded-99e9-b607d29abaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# num_labels = 5\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\", \n",
    "#     num_labels=num_labels, \n",
    "#     problem_type=\"multi_label_classification\",\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3364640-a01c-40af-834a-2dc5f42aeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import PegasusTokenizerFast, BigBirdPegasusForSequenceClassification\n",
    "\n",
    "# tokenizer = PegasusTokenizerFast.from_pretrained(\"hf-internal-testing/tiny-random-bigbird_pegasus\")\n",
    "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "# num_labels = 5\n",
    "# model = BigBirdPegasusForSequenceClassification.from_pretrained(\n",
    "#     \"hf-internal-testing/tiny-random-bigbird_pegasus\", \n",
    "#     num_labels=num_labels, problem_type=\"multi_label_classification\", \n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "\n",
    "# # labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(\n",
    "# #     torch.float\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d017b4-92fa-456c-81ab-9de38bf69a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128f4fe-b78f-484f-bdb8-924144a9d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    text = examples[\"text\"]\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx]= labels_batch[label]\n",
    "    \n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d7861-f929-486c-b80c-190892dc6499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459b5eccfdc346df865b9499ef16d005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484360c883c4b699d88d7823ac3a3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7462e2-81d7-4fd6-84ca-e503e348c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 121\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdce56-f07f-4596-be35-f47760af2f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset[\"train\"]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca4dc6-b3f1-4dc5-b4a4-06cddac756f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = encoded_dataset['train'][3]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0581864-2cd4-4b6a-96bd-47a5b97763b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>How to Test Your Startup Idea for \\t\\t username username username username username You ve been quoted in my Storify story New Story \\t\\tNew Story storify caceroludos\\t\\t username username username username You ve been quoted in my Storify story caceroludos \\t\\t username username username username username You ve been quoted in my Storify story caceroludos \\t\\t username You ve been quoted in my Storify story caceroludos \\t\\tBusiness models are a commodity Stop asking But how will they make money \\t\\tY Combinator Alum Flutter Raises Million For Gesture Recognition Tech \\t\\tMontreal Protests Months In photos The Atlantic \\t\\t I don t want to archieve immortality through my work I want to achieve it through not dying Woody Allen\\t\\tPor comer en la oficina d Lvk username St Andrews \\t\\t username take it easy \\t\\tI m at Aeroparque Jorge Newbery AEP Ciudad de Buenos Aires w others \\t\\t username your Klout Score is decent I m a Check out your username profile today \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\t username a torrar \\t\\tI m at Aeroparque Jorge Newbery AEP w username username \\t\\tI just unlocked the Swarm badge on username \\t\\tJust posted a photo \\t\\tLos yankis bailan cuarteto pero despasito p username Edinburgh Castle w others \\t\\tA lot people with pride \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tI m at Powell Street Cable Car Turnaround San Francisco CA w others \\t\\tI m at San Francisco Pride San Francisco CA w others \\t\\tI m at Civic Center Plaza San Francisco CA w others \\t\\tJust posted a photo \\t\\tSmirnoff girls \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tI m at HI San Francisco City Center San Francisco CA w others \\t\\tJust posted a photo \\t\\tNaked men all over the pride \\t\\tJust posted a photo \\t\\tAll over the city \\t\\t Go to R Bar Get a group of friends and take a Fernet shot username R Bar via Anna M \\t\\tJust posted a photo \\t\\tGeeky meeting near the gate \\t\\tI m at Startups Mountain View CA w others \\t\\tI m at Google HQ Mountain View Ca \\t\\tI m at Googleplex Google Store Mountain View CA \\t\\tI m at Googleplex Steam Cafe Mountain View CA \\t\\tI m at Google Android Camp Mountain View CA w others \\t\\tI m at Computer History Museum Mountain View CA \\t\\tHard disk of late It still works \\t\\t username thanks for receiving us this afternoon u r very cool people and inspired us to follow your steps\\t\\tGood morning SF \\t\\tGoogle Engage username Googleplex \\t\\tI m at Googleplex Long Life Mountain View CA w others \\t\\tJelly bean it s here username Google Android Camp w others \\t\\tPeople of username rocking at google \\t\\tIt s a Startup world username google gdg \\t\\tI m at Google I O w others \\t\\t Only percent of USA citizen has passport this is awful truth \\t\\tJust posted a photo \\t\\tI m at Pancho Villa Taqueria San Francisco CA w others \\t\\tI m at Dalva San Francisco CA w others \\t\\t Mission bar Dalva was named after a a book of the same name by Jim Harrison username Dalva via usernamescoutmob foursquare\\t\\t SF Weekly’s Best Of Winner – Editorial Pick – BEST BAR INSIDE A BAR username Dalva via usernamesfweeklystreet foursquare\\t\\tI m at Mashery San Francisco CA w others \\t\\tI m at Mashery San Francisco CA w others \\t\\tI m at Off the Grid UN Plaza San Francisco California \\t\\tI m at Apple Store San Francisco CA w others \\t\\tI m at Hilton San Francisco Union Square San Francisco CA w others \\t\\tI m at Jasper s Corner Tap amp Kitchen San Francisco CA \\t\\tI m at Helen Wills Playground San Francisco CA \\t\\tI m at Lombard Hyde Cable Car Stop \\t\\tI m at Fisherman s Wharf Sign San Francisco CA w others \\t\\tI m at Bubba Gump Shrimp Co San Francisco CA w others \\t\\tI m at Apple Store San Francisco CA w others \\t\\tI m at Disney Store San Francisco CA \\t\\tI m at Code for America San Francisco CA w others \\t\\tI just unlocked the Greasy Spoon badge on username for checking in at diners Bring on the burgers \\t\\tI m at Pinecrest Diner San Francisco CA \\t\\tI m at HI San Francisco City Center San Francisco CA \\t\\t of J gringo party username Cova Hotel w others \\t\\tI m at Bigfoot Lodge San Francisco CA \\t\\tI just unlocked the Great Outdoors badge on username for checking in at outdoor spots Freedom \\t\\tI m at Chinatown Gate San Francisco CA w others \\t\\tI m at Chinatown San Francisco CA w others \\t\\tI m at First public school In California San Francisco CA \\t\\tI m at Sue Bierman Park San Francisco CA \\t\\tI m at Vaillancourt Fountain San Francisco CA \\t\\tI m at Embarcadero Center Walkway San Francisco CA \\t\\tGrain salad salmon and Heretic evil twin beer username Jasper s Corner Tap amp Kitchen w others \\t\\tI m at Chinatown San Francisco CA \\t\\tI just unlocked the Overshare badge on username \\t\\tI m at Chinatown Gate San Francisco CA \\t\\tI m at GitHub HQ San Francisco CA \\t\\tI m at Street Plaza San Francisco CA \\t\\tI m at Los Compadres San Francisco CA w others \\t\\tI m at South Park San Francisco CA w others \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tJust posted a photo \\t\\tI m at The Women s Center San Francisco CA \\t\\tI m at Bi Rite Creamery San Francisco CA w others \\t\\t</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b585f56-3443-464a-8b91-ac8b12a6ffdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efdc1c-e716-407c-87fc-8227af737aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['extroverted', 'stable', 'agreeable', 'open', 'conscientious']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9325f-a539-4706-ac18-ee8127a8d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539e00d-8c48-487d-b72f-18f9de97cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset = (encoded_dataset\n",
    "#           .map(lambda x : {\"float_labels\": x[\"labels\"].to(torch.float)}, remove_columns=[\"labels\"])\n",
    "#           .rename_column(\"float_labels\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e803280-6887-4899-a81b-aba2d22ec476",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3898cb-975a-4a5e-bea5-43970fc8f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bigbert-fineturned-pandataset\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8a814-47a5-487c-b37f-00ef7c35781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438038b4-265a-4107-a0b6-139ef69d17ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01774726-5b91-4a82-8094-b11cd4c2ae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  4030, 13019,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf517200-dab1-41c5-96a0-7cbbe892d171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd41846-9b54-4d1d-9cf1-0393583a77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d3368-ed49-4f22-b5e5-d17606c2f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 121\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 80\n",
      "Initializing global attention on CLS token...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04119ef5-b270-4cb9-bff4-6addc7a3272f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
